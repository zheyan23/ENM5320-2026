<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Fundamentals - ENM5320</title>
    
    <!-- KaTeX CSS and JS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #fff;
        }

        h1 {
            color: #011F5B;
            border-bottom: 3px solid #990000;
            padding-bottom: 10px;
            margin-top: 30px;
        }

        h2 {
            color: #011F5B;
            margin-top: 30px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 5px;
        }

        h3 {
            color: #011F5B;
            margin-top: 20px;
        }

        p {
            text-align: justify;
            margin-bottom: 15px;
        }

        ul, ol {
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        .katex-display {
            margin: 1.5em 0;
            overflow-x: auto;
            overflow-y: hidden;
        }

        .definition {
            background: #f8f9fa;
            border-left: 4px solid #82AFD3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .summary-box {
            background: #fff3cd;
            border: 1px solid #F2C100;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .example-box {
            background: #e8f4f8;
            border-left: 4px solid #82AFD3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .theorem-box {
            background: #f0f0f0;
            border: 2px solid #011F5B;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        strong {
            color: #011F5B;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            color: #82AFD3;
            text-decoration: none;
        }

        .back-link:hover {
            text-decoration: underline;
        }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <a href="../../index.html" class="back-link">← Back to Course Schedule</a>
    
    <h1>Mathematical Fundamentals</h1>

    <p>This document provides a summary of mathematical background that will be assumed throughout the course. It is incomplete - the following references will be useful if you need to fill in the gaps.</p>

    <ul>
        <li><a href="https://find.library.upenn.edu/catalog/9977066647603681?hld_id=53791354980003681" target="_blank">Gene Golub & Charles Van Loan, <em>Matrix Computations</em> (4th Edition)</a> - Available online through Penn Library. The linear algebra bible!</li>
        <li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" target="_blank">The Matrix Cookbook</a> - A PDF collecting essentially all matrix/vector identities in one spot beyond those here.</li>
    </ul>

    <p>These tools will provide the machinery we need to talk about stability and prove properties of machine learning architectures. If this is new to you, it's OK to be a little overwhelmed. We will work examples and get plenty of practice using these over and over throughout the semester. If you need help filling in gaps in your math background, be proactive and come to office hours early. Mastering these kinds of manipulations is like learning a foreign language or a musical instrument - it takes practice, and if you wait until the day before assignments/tests you won't have enough time to get these into your mental muscle memory.   </p>

    <h2>Vector Norms</h2>

    <div class="definition">
        <strong>Definition (Norm):</strong> Let \(\mathbf{x} \in \mathbb{R}^n\). A <strong>norm</strong> \(\|\cdot\|\) is a function \(\mathbb{R}^n \to \mathbb{R}_+\) satisfying:
        <ol>
            <li>\(\|\mathbf{x}\| \geq 0\) with equality if and only if \(\mathbf{x} = \mathbf{0}\) (positive definiteness)</li>
            <li>\(\|\alpha \mathbf{x}\| = |\alpha| \|\mathbf{x}\|\) for all \(\alpha \in \mathbb{R}\) (homogeneity)</li>
            <li>\(\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|\) (triangle inequality)</li>
        </ol>
    </div>

    <h3>Common Vector Norms</h3>

    <p><strong>\(\ell^p\) norms:</strong> For \(p \geq 1\),</p>
    $$\|\mathbf{x}\|_p = \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}$$

    <p>Important special cases:</p>
    <ul>
        <li><strong>\(\ell^1\) norm (Manhattan):</strong> \(\|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|\)</li>
        <li><strong>\(\ell^2\) norm (Euclidean):</strong> \(\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{\mathbf{x}^\top \mathbf{x}}\)</li>
        <li><strong>\(\ell^\infty\) norm (max norm):</strong> \(\|\mathbf{x}\|_\infty = \max_{i=1,\ldots,n} |x_i|\)</li>
    </ul>

    <h3>Inner Product Induced Norms</h3>

    <p>Every inner product induces a norm via:</p>
    $$\|\mathbf{x}\| = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}$$

    <p>This norm satisfies all three norm axioms. The \(\ell^2\) norm is precisely the norm induced by the standard inner product:</p>
    $$\|\mathbf{x}\|_2 = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle} = \sqrt{\sum_{i=1}^n x_i^2}$$

    <p>For a weighted inner product \(\langle \mathbf{x}, \mathbf{y} \rangle_M = \mathbf{x}^\top M \mathbf{y}\) with \(M\) symmetric positive definite, the induced norm is:</p>
    $$\|\mathbf{x}\|_M = \sqrt{\mathbf{x}^\top M \mathbf{x}}$$

    <p>This is often called an <strong>energy norm</strong> or <strong>\(M\)-norm</strong>.</p>

    <div class="summary-box">
        <strong>Important:</strong> Not all norms come from inner products. A norm is induced by an inner product if and only if it satisfies the parallelogram law:
        $$\|\mathbf{x} + \mathbf{y}\|^2 + \|\mathbf{x} - \mathbf{y}\|^2 = 2\|\mathbf{x}\|^2 + 2\|\mathbf{y}\|^2$$
        <p>For example, the \(\ell^1\) and \(\ell^\infty\) norms are <strong>not</strong> induced by any inner product.</p>
    </div>

    <h3>Norm Equivalence</h3>

    <p>In finite dimensions, all norms are equivalent. Specifically, for any two norms \(\|\cdot\|_a\) and \(\|\cdot\|_b\) on \(\mathbb{R}^n\), there exist constants \(c, C > 0\) such that</p>
    $$c \|\mathbf{x}\|_a \leq \|\mathbf{x}\|_b \leq C \|\mathbf{x}\|_a \quad \text{for all } \mathbf{x} \in \mathbb{R}^n$$

    <p>Concrete examples:</p>
    $$\|\mathbf{x}\|_\infty \leq \|\mathbf{x}\|_2 \leq \sqrt{n} \|\mathbf{x}\|_\infty$$
    $$\|\mathbf{x}\|_2 \leq \|\mathbf{x}\|_1 \leq \sqrt{n} \|\mathbf{x}\|_2$$

    <h2>Inner Products</h2>

    <div class="definition">
        <strong>Definition (Inner Product):</strong> An <strong>inner product</strong> on \(\mathbb{R}^n\) is a function \(\langle \cdot, \cdot \rangle : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}\) satisfying:
        <ol>
            <li>\(\langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle\) (symmetry)</li>
            <li>\(\langle \alpha \mathbf{x} + \beta \mathbf{y}, \mathbf{z} \rangle = \alpha \langle \mathbf{x}, \mathbf{z} \rangle + \beta \langle \mathbf{y}, \mathbf{z} \rangle\) (linearity in first argument)</li>
            <li>\(\langle \mathbf{x}, \mathbf{x} \rangle \geq 0\) with equality if and only if \(\mathbf{x} = \mathbf{0}\) (positive definiteness)</li>
        </ol>
    </div>

    <h3>Standard Inner Product</h3>

    <p>The <strong>standard inner product</strong> (or <strong>Euclidean inner product</strong>) on \(\mathbb{R}^n\) is:</p>
    $$\langle \mathbf{x}, \mathbf{y} \rangle = \sum_{i=1}^n x_i y_i = \mathbf{x}^\top \mathbf{y}$$

    <p>This can also be written in Einstein notation as \(\langle \mathbf{x}, \mathbf{y} \rangle = x_i y_i\).</p>

    <h3>Weighted Inner Products</h3>

    <p>More generally, given a symmetric positive definite matrix \(M \in \mathbb{R}^{n \times n}\), we can define a weighted inner product:</p>
    $$\langle \mathbf{x}, \mathbf{y} \rangle_M = \mathbf{x}^\top M \mathbf{y} = \sum_{i,j=1}^n x_i M_{ij} y_j$$

    <p>The standard inner product corresponds to \(M = I\) (the identity matrix).</p>

    <h3>Properties</h3>

    <p>Key properties of inner products:</p>

    <p><strong>Cauchy-Schwarz inequality:</strong></p>
    $$|\langle \mathbf{x}, \mathbf{y} \rangle| \leq \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle} \sqrt{\langle \mathbf{y}, \mathbf{y} \rangle}$$

    <p><strong>Polarization identity:</strong></p>
    $$\langle \mathbf{x}, \mathbf{y} \rangle = \frac{1}{4}\left(\|\mathbf{x} + \mathbf{y}\|^2 - \|\mathbf{x} - \mathbf{y}\|^2\right)$$

    <p>where \(\|\mathbf{x}\| = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}\) is the induced norm.</p>

    <p><strong>Parallelogram law:</strong></p>
    $$\|\mathbf{x} + \mathbf{y}\|^2 + \|\mathbf{x} - \mathbf{y}\|^2 = 2\|\mathbf{x}\|^2 + 2\|\mathbf{y}\|^2$$

    <h2>Matrix Norms</h2>

    <p>For \(A \in \mathbb{R}^{m \times n}\), an <strong>induced (operator) norm</strong> is defined as</p>
    $$\|A\| = \sup_{\mathbf{x} \neq \mathbf{0}} \frac{\|A\mathbf{x}\|}{\|\mathbf{x}\|} = \sup_{\|\mathbf{x}\| = 1} \|A\mathbf{x}\|$$

    <h3>Common Matrix Norms</h3>

    <ul>
        <li><strong>Induced \(\ell^1\) norm:</strong> \(\|A\|_1 = \max_{j=1,\ldots,n} \sum_{i=1}^m |a_{ij}|\) (max column sum)</li>
        <li><strong>Induced \(\ell^2\) norm (spectral norm):</strong> \(\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)} = \sigma_{\max}(A)\) (largest singular value)</li>
        <li><strong>Induced \(\ell^\infty\) norm:</strong> \(\|A\|_\infty = \max_{i=1,\ldots,m} \sum_{j=1}^n |a_{ij}|\) (max row sum)</li>
        <li><strong>Frobenius norm:</strong> \(\|A\|_F = \sqrt{\sum_{i,j} a_{ij}^2} = \sqrt{\text{tr}(A^\top A)}\) (not induced, but submultiplicative)</li>
    </ul>

    <h3>Properties of Matrix Norms</h3>

    <p>For any induced matrix norm:</p>
    <ol>
        <li><strong>Submultiplicativity:</strong> \(\|AB\| \leq \|A\| \|B\|\)</li>
        <li><strong>Consistency:</strong> \(\|A\mathbf{x}\| \leq \|A\| \|\mathbf{x}\|\)</li>
        <li>\(\|I\| = 1\) where \(I\) is the identity matrix</li>
    </ol>

    <p>Additional useful properties:</p>
    <ul>
        <li>\(\|A\|_2 \leq \sqrt{\|A\|_1 \|A\|_\infty}\)</li>
        <li>\(\|A\|_2 \leq \|A\|_F \leq \sqrt{\min(m,n)} \|A\|_2\)</li>
    </ul>

    <h2>Einstein Summation Convention and Tensor Notation</h2>

    <h3>Einstein Notation</h3>

    <p>The <strong>Einstein summation convention</strong> automatically sums over repeated indices that appear once as a subscript and once as a superscript (or twice as subscripts in Cartesian coordinates). This eliminates the need for explicit summation symbols.</p>

    <p><strong>Basic rules:</strong></p>
    <ul>
        <li>Repeated indices are summed over (called <strong>dummy indices</strong>)</li>
        <li>Free indices must match on both sides of an equation</li>
        <li>Each index appears at most twice in any term</li>
    </ul>

    <div class="example-box">
        <strong>Examples:</strong>
        
        <p>Inner product of vectors \(\mathbf{u}, \mathbf{v} \in \mathbb{R}^n\):</p>
        $$\mathbf{u} \cdot \mathbf{v} = u_i v_i = \sum_{i=1}^n u_i v_i$$

        <p>Matrix-vector multiplication \((A\mathbf{x})_i\):</p>
        $$y_i = A_{ij} x_j = \sum_{j=1}^n A_{ij} x_j$$

        <p>Matrix-matrix multiplication \((AB)_{ik}\):</p>
        $$C_{ik} = A_{ij} B_{jk} = \sum_{j=1}^n A_{ij} B_{jk}$$

        <p>Trace of a matrix:</p>
        $$\text{tr}(A) = A_{ii} = \sum_{i=1}^n A_{ii}$$

        <p>Frobenius norm squared:</p>
        $$\|A\|_F^2 = A_{ij} A_{ij} = \sum_{i,j} A_{ij}^2$$
    </div>

    <h3>Kronecker Delta</h3>

    <div class="definition">
        <strong>Definition (Kronecker Delta):</strong> The <strong>Kronecker delta</strong> \(\delta_{ij}\) is defined as:
        $$\delta_{ij} = \begin{cases} 
        1 & \text{if } i = j \\
        0 & \text{if } i \neq j
        \end{cases}$$
        <p>In matrix form, \([\delta_{ij}] = I\) (the identity matrix).</p>
    </div>

    <p><strong>Properties:</strong></p>
    <ul>
        <li>\(\delta_{ij} v_j = v_i\) (extracts the \(i\)-th component)</li>
        <li>\(\delta_{ii} = n\) (sum over a repeated index gives the dimension)</li>
        <li>\(\delta_{ij} \delta_{jk} = \delta_{ik}\) (composition property)</li>
        <li>\(A_{ij} \delta_{jk} = A_{ik}\) (relabeling index)</li>
    </ul>

    <h3>Levi-Civita Symbol</h3>

    <div class="definition">
        <strong>Definition (Levi-Civita Symbol):</strong> The <strong>Levi-Civita symbol</strong> (or <strong>permutation tensor</strong>) \(\epsilon_{ijk}\) in 3D is defined as:
        $$\epsilon_{ijk} = \begin{cases} 
        +1 & \text{if } (i,j,k) \text{ is an even permutation of } (1,2,3) \\
        -1 & \text{if } (i,j,k) \text{ is an odd permutation of } (1,2,3) \\
        0 & \text{if any index is repeated}
        \end{cases}$$
    </div>

    <p><strong>Even permutations:</strong> \((1,2,3), (2,3,1), (3,1,2)\) have \(\epsilon = +1\)</p>
    <p><strong>Odd permutations:</strong> \((1,3,2), (3,2,1), (2,1,3)\) have \(\epsilon = -1\)</p>

    <p><strong>Applications:</strong></p>

    <p>Cross product in \(\mathbb{R}^3\):</p>
    $$(\mathbf{u} \times \mathbf{v})_i = \epsilon_{ijk} u_j v_k$$

    <p>Explicitly:</p>
    $$\mathbf{u} \times \mathbf{v} = \begin{pmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{pmatrix}$$

    <p>Determinant of a \(3 \times 3\) matrix:</p>
    $$\det(A) = \epsilon_{ijk} A_{1i} A_{2j} A_{3k}$$

    <p><strong>Contraction identity</strong> (very useful):</p>
    $$\epsilon_{ijk} \epsilon_{lmk} = \delta_{il} \delta_{jm} - \delta_{im} \delta_{jl}$$

    <p><strong>Vector identity:</strong></p>
    $$(\mathbf{u} \times \mathbf{v}) \cdot (\mathbf{w} \times \mathbf{z}) = (\mathbf{u} \cdot \mathbf{w})(\mathbf{v} \cdot \mathbf{z}) - (\mathbf{u} \cdot \mathbf{z})(\mathbf{v} \cdot \mathbf{w})$$

    <p>which can be proven using: \(\epsilon_{ijk} \epsilon_{imn} = \delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}\)</p>

    <h2>Fundamental Inequalities</h2>

    <h3>Cauchy-Schwarz Inequality</h3>

    <p>For \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\):</p>
    $$|\mathbf{x}^\top \mathbf{y}| \leq \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$$

    <p>Equality holds if and only if \(\mathbf{x}\) and \(\mathbf{y}\) are linearly dependent.</p>

    <p><strong>Generalization:</strong> For any inner product space,</p>
    $$|\langle \mathbf{x}, \mathbf{y} \rangle| \leq \|\mathbf{x}\| \|\mathbf{y}\|$$

    <h3>Triangle Inequality</h3>

    <p>For any norm:</p>
    $$\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|$$

    <p><strong>Reverse triangle inequality:</strong></p>
    $$\big| \|\mathbf{x}\| - \|\mathbf{y}\| \big| \leq \|\mathbf{x} - \mathbf{y}\|$$

    <h3>Hölder's Inequality</h3>

    <p>For \(p, q \geq 1\) with \(\frac{1}{p} + \frac{1}{q} = 1\) (conjugate exponents):</p>
    $$\sum_{i=1}^n |x_i y_i| \leq \|\mathbf{x}\|_p \|\mathbf{y}\|_q$$

    <p>Special case (\(p = q = 2\)): Cauchy-Schwarz inequality.</p>

    <h3>Minkowski's Inequality</h3>

    <p>For \(p \geq 1\):</p>
    $$\|\mathbf{x} + \mathbf{y}\|_p \leq \|\mathbf{x}\|_p + \|\mathbf{y}\|_p$$

    <p>This is the triangle inequality for \(\ell^p\) norms.</p>

    <h3>Young's Inequality</h3>

    <p>For \(a, b \geq 0\) and conjugate exponents \(p, q > 1\) with \(\frac{1}{p} + \frac{1}{q} = 1\):</p>
    $$ab \leq \frac{a^p}{p} + \frac{b^q}{q}$$

    <p>Special case (\(p = q = 2\)):</p>
    $$ab \leq \frac{a^2}{2} + \frac{b^2}{2}$$

    <p>More generally, for any \(\epsilon > 0\):</p>
    $$ab \leq \frac{\epsilon a^2}{2} + \frac{b^2}{2\epsilon}$$

    <p>This is particularly useful for absorbing terms in energy estimates.</p>

    <h2>Useful Manipulation Techniques</h2>

    <h3>Completing the Square</h3>

    <p>For scalar \(a, b\) and \(\epsilon > 0\):</p>
    $$2ab = \epsilon a^2 + \frac{b^2}{\epsilon} - \left(\sqrt{\epsilon} a - \frac{b}{\sqrt{\epsilon}}\right)^2 \leq \epsilon a^2 + \frac{b^2}{\epsilon}$$

    <p>For vectors with respect to a positive definite matrix \(M\):</p>
    $$2\mathbf{x}^\top M \mathbf{y} \leq \epsilon \mathbf{x}^\top M \mathbf{x} + \frac{1}{\epsilon} \mathbf{y}^\top M \mathbf{y}$$

    <h3>Gronwall's Inequality</h3>

    <p>If \(u(t) \geq 0\) satisfies</p>
    $$u(t) \leq C + \int_0^t \alpha(s) u(s) \, ds$$
    <p>for constants \(C \geq 0\) and \(\alpha(s) \geq 0\), then</p>
    $$u(t) \leq C \exp\left(\int_0^t \alpha(s) \, ds\right)$$

    <p><strong>Discrete version:</strong> If \(u_n \geq 0\) satisfies \(u_n \leq C + \sum_{k=0}^{n-1} \alpha_k u_k\) with \(\alpha_k \geq 0\), then</p>
    $$u_n \leq C \prod_{k=0}^{n-1} (1 + \alpha_k)$$

    <h3>Matrix Spectral Properties</h3>

    <p>For a symmetric matrix \(A \in \mathbb{R}^{n \times n}\):</p>

    <p><strong>Rayleigh quotient:</strong></p>
    $$\lambda_{\min}(A) \leq \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}} \leq \lambda_{\max}(A)$$

    <p><strong>Weyl's inequality:</strong> For symmetric \(A, B\):</p>
    $$\lambda_{\min}(A) + \lambda_{\min}(B) \leq \lambda_{\min}(A + B) \leq \lambda_{\min}(A) + \lambda_{\max}(B)$$

    <p><strong>Energy inequality:</strong> If \(A\) is positive definite with \(\lambda_{\min}(A) = \lambda > 0\):</p>
    $$\lambda \|\mathbf{x}\|_2^2 \leq \mathbf{x}^\top A \mathbf{x} \leq \lambda_{\max}(A) \|\mathbf{x}\|_2^2$$

    <h2>Summary of Common Estimates</h2>

    <div class="summary-box">
        <strong>Quick reference for frequently used bounds:</strong>
        <ol>
            <li>\(|\mathbf{x}^\top \mathbf{y}| \leq \|\mathbf{x}\|_2 \|\mathbf{y}\|_2\) (Cauchy-Schwarz)</li>
            <li>\(\|A\mathbf{x}\|_2 \leq \|A\|_2 \|\mathbf{x}\|_2\) (consistency)</li>
            <li>\(\|AB\|_2 \leq \|A\|_2 \|B\|_2\) (submultiplicativity)</li>
            <li>\(ab \leq \frac{\epsilon a^2}{2} + \frac{b^2}{2\epsilon}\) for any \(\epsilon > 0\) (Young)</li>
            <li>\(\|\mathbf{x}\|_\infty \leq \|\mathbf{x}\|_2 \leq \sqrt{n} \|\mathbf{x}\|_\infty\) (norm equivalence)</li>
            <li>\(\|A\|_2 \leq \|A\|_F \leq \sqrt{n} \|A\|_2\) (for \(A \in \mathbb{R}^{n \times n}\))</li>
        </ol>
        <p>These tools form the foundation for deriving stability estimates, convergence rates, and error bounds throughout the course.</p>
    </div>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>
</body>
</html>

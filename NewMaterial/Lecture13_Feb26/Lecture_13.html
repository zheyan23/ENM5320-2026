<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture 13: Quasi-Optimality, Error Estimation, and Lax-Milgram Theory - ENM5320</title>
    
    <!-- KaTeX CSS and JS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #fff;
        }

        h1 {
            color: #011F5B;
            border-bottom: 3px solid #990000;
            padding-bottom: 10px;
            margin-top: 30px;
        }

        h2 {
            color: #011F5B;
            margin-top: 30px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 5px;
        }

        h3 {
            color: #011F5B;
            margin-top: 20px;
        }

        p {
            text-align: justify;
            margin-bottom: 15px;
        }

        ul, ol {
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        .katex-display {
            margin: 1.5em 0;
            overflow-x: auto;
            overflow-y: hidden;
        }

        .definition {
            background: #f8f9fa;
            border-left: 4px solid #82AFD3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .summary-box {
            background: #fff3cd;
            border: 1px solid #F2C100;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .example-box {
            background: #e8f4f8;
            border-left: 4px solid #82AFD3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .theorem-box {
            background: #f0f0f0;
            border: 2px solid #011F5B;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        strong {
            color: #011F5B;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            color: #82AFD3;
            text-decoration: none;
        }

        .back-link:hover {
            text-decoration: underline;
        }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
        }

        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
    </style>
</head>
<body>
    <a href="../../index.html" class="back-link">← Back to Course Schedule</a>
    
    <h1>Lecture 13: Quasi-Optimality, Error Estimation, and Lax-Milgram Theory</h1>

    <p><strong>Date:</strong> March 19</p>
    <p><strong>Topics Covered:</strong> Quasi-optimality in energy norm, Error estimation in $L^2$ norm via duality argument, Interpolation theory and approximation bounds, Lax-Milgram theory for general elliptic problems</p>
    <p><strong>References:</strong> Johnson, Brenner & Scott</p>

    <hr>

    <h2>0. Overview</h2>
    <p>In the previous lecture, we introduced the **Galerkin discretization** of the Poisson problem and established Galerkin orthogonality. Today, we take a major step toward understanding **why finite element methods work so well**: we'll prove rigorous error estimates that show how quickly FEM solutions converge to the true solution as we refine our mesh. These aren't just numerical observations—they're mathematical guarantees.</p>

    <p>We begin by generalizing our Galerkin formulation to **abstract bilinear forms**, which will allow us to handle a much broader class of PDEs. The key concept is the **energy norm** induced by the bilinear form, which naturally connects the Galerkin method to the Rayleigh-Ritz energy minimization principle we saw before. We'll prove **quasi-optimality**: the FEM solution is the best approximation in the chosen function space, measured in the energy norm.</p>

    <p>But what about the $L^2$ norm, the most natural measure of error? Through an elegant **duality argument** (sometimes called Aubin-Nitsche trick), we'll show that $L^2$ error converges **twice as fast** as energy error—a phenomenon called **superconvergence**. The proof hinges on constructing an auxiliary problem and leveraging interpolation theory. Finally, we'll abstract these ideas through **Lax-Milgram theory**, which provides a unified framework for proving existence, uniqueness, and stability for any elliptic bilinear form. This theory is the foundation for learning both function spaces $V_h$ and operators $a(u,v)$ in physics-informed machine learning.</p>

    <hr>

    <h2>1. Bilinear Forms and Energy Norms</h2>
    
    <h3>1.1 Generalization to Abstract Bilinear Forms</h3>
    <p>Last time we introduced the Galerkin discretization of the Poisson problem:</p>
    <p>$$(G) \quad (\nabla u, \nabla v) = (f, v) \quad \text{for all } v \in V_h$$</p>

    <p>We will now consider this as an example of a **general bilinear form** $a(\cdot, \cdot)$:</p>
    <p>$$a(u, v) = (f, v)$$</p>

    <div class="definition">
        <p><strong>Definition:</strong> A bilinear form $a: V \times V \to \mathbb{R}$ satisfies:</p>
        <p>$$\begin{aligned}
        a(\alpha u_1 + \beta u_2, \gamma v_1 + \delta v_2) &= \alpha \gamma a(u_1, v_1) + \alpha \delta a(u_1, v_2) \\
        &\quad + \beta \gamma a(u_2, v_1) + \beta \delta a(u_2, v_2)
        \end{aligned}$$</p>
        <p>(linear in both arguments)</p>
    </div>

    <h3>1.2 Energy Norm and Cauchy-Schwarz</h3>
    <p>For bilinear forms, we're interested in those that **generate an energy norm** with a Cauchy-Schwarz inequality:</p>
    <p>$$\boxed{\|v\|_E = \sqrt{a(v,v)}}$$</p>
    <p>$$\boxed{a(v,w) \leq \|v\|_E \|w\|_E}$$</p>

    <p><strong>Recall from last class:</strong> This is precisely the connection between Galerkin and Rayleigh-Ritz:</p>
    <p>$$\text{Solving } a(u,v) = (f,v) \quad \Leftrightarrow \quad \min_{v \in V_h} \|v\|_E^2$$</p>

    <h3>1.3 Galerkin Orthogonality (Recap)</h3>
    <p>Last class we showed from **Galerkin orthogonality**:</p>
    <p>$$\|u - u_h\|_E \leq \|u - v\|_E \quad \forall v \in V_h$$</p>

    <p>or taking the infimum over $V_h$:</p>
    <p>$$\boxed{\|u - u_h\|_E \leq \inf_{v \in V_h} \|u - v\|_E}$$</p>

    <p><strong>This is quasi-optimality:</strong> The FEM error is bounded by the best approximation error in $V_h$.</p>

    <hr>

    <h2>2. Error in $L^2$ Norm: The Duality Argument</h2>
    
    <h3>2.1 From Energy Norm to $L^2$ Norm</h3>
    <p>We know that $(G)$ naturally minimizes error in the **energy norm**. What about the **$L^2$ norm**?</p>
    <p>$$\|u - u_h\|^2 = \int_\Omega (u - u_h)^2 \, dx$$</p>

    <p><strong>We'll show:</strong> $L^2$ error is smaller than energy error, following a **duality argument** (Aubin-Nitsche trick).</p>

    <h3>2.2 The Auxiliary Problem</h3>
    <p>Define a new problem:</p>
    <p>$$\begin{cases}
    -w'' = u - u_h \\
    w(0) = w(1) = 0
    \end{cases}$$</p>

    <p><strong>Key idea:</strong> Use $w$ to relate $L^2$ error to energy error.</p>

    <h3>2.3 The Duality Argument</h3>
    <p><strong>Step 1:</strong> Start with $L^2$ norm squared:</p>
    <p>$$\begin{aligned}
    \|u - u_h\|^2 &= (u - u_h, u - u_h) \\
    &= (u - u_h, -w'')
    \end{aligned}$$</p>

    <p><strong>Step 2:</strong> Integrate by parts:</p>
    <p>$$\begin{aligned}
    &= (u' - u_h', w') + \underbrace{[w'(0)(u - u_h)(0) - w'(1)(u - u_h)(1)]}_{= 0 \text{ by BCs}} \\
    &= a(u - u_h, w)
    \end{aligned}$$</p>

    <p><strong>Step 3:</strong> Apply Galerkin orthogonality (for any $v \in V_h$):</p>
    <p>$$= a(u - u_h, w - v)$$</p>

    <p><strong>Step 4:</strong> Apply Cauchy-Schwarz for the bilinear form:</p>
    <p>$$\leq \|u - u_h\|_E \|w - v\|_E$$</p>

    <p><strong>Step 5:</strong> Divide both sides by $\|u - u_h\|$ and use $-w'' = u - u_h$:</p>
    <p>$$\|u - u_h\| \leq \frac{\|u - u_h\|_E \|w - v\|_E}{\|u - u_h\|} = \frac{\|u - u_h\|_E \|w - v\|_E}{\|-w''\|}$$</p>

    <p><strong>Step 6:</strong> Take infimum over $v \in V_h$:</p>
    <p>$$\boxed{\|u - u_h\| \leq \|u - u_h\|_E \inf_{v \in V_h} \frac{\|w - v\|_E}{\|w''\|}}$$</p>

    <h3>2.4 The Key Approximation Property</h3>
    <p><strong>If we can find $v \in V_h$ such that:</strong></p>
    <p>$$\|w - v\|_E \leq \varepsilon \|w''\| \tag{A}$$</p>

    <p><strong>Then:</strong></p>
    <p>$$\|u - u_h\| \leq \varepsilon \|u - u_h\|_E$$</p>

    <p><strong>Applying quasi-optimality again:</strong></p>
    <p>$$\boxed{\|u - u_h\| \leq \varepsilon^2 \|u''\| = \varepsilon^2 \|f\|}$$</p>

    <p><strong>Everything comes down to showing that $V_h$ can approximate $w$ well enough!</strong></p>
    <p>For piecewise linear choice of $V_h$, we'll show what $\varepsilon$ is.</p>

    <hr>

    <h2>3. Interpolation Theory</h2>
    
    <h3>3.1 Setup</h3>
    <p>Define nodes: $0 = x_0 < x_1 < \ldots < x_n = 1$</p>
    <p>$$V_h = \{v \in C^0[0,1] : v|_{[x_{i-1}, x_i]} \text{ is a linear polynomial}, v(0) = 0\}$$</p>

    <p>Define **nodal basis functions** $\phi_i(x)$, $i = 1, \ldots, n$ satisfying:</p>
    <p>$$\phi_i(x_j) = \delta_{ij} = \begin{cases}
    1 & i = j \\
    0 & i \neq j
    \end{cases}$$</p>
    <p><em>(Kronecker-$\delta$ property)</em></p>

    <h3>3.2 The Interpolant</h3>
    <p>Define the **interpolant** $\pi u \in V_h$ satisfying:</p>
    <p>$$\pi u(x_i) = u(x_i) \quad \forall \text{ nodes } x_i$$</p>

    <p><strong>This can be computed directly:</strong></p>
    <p>$$\begin{aligned}
    \pi u(x_i) &= \sum_j \widehat{\pi u}_j \phi_j(x_i) = u(x_i) \\
    &= \sum_j \widehat{\pi u}_j \delta_{ij} \\
    &= \widehat{\pi u}_i
    \end{aligned}$$</p>

    <p><strong>Therefore:</strong></p>
    <p>$$\boxed{\pi u(x) = \sum_j u(x_j) \phi_j(x)}$$</p>

    <h3>3.3 The Interpolation Error Theorem</h3>
    <p>We'll show that $\pi u$ is the $v$ we needed in $(A)$.</p>

    <div class="theorem-box">
        <p><strong>Theorem:</strong> For all $u \in V$ with $u'' \in L^2$:</p>
        <p>$$\boxed{\|u - \pi u\|_E \leq Ch \|u''\|}$$</p>
        <p>where $C$ is independent of $h$ and $u$.</p>
        <p><strong>This is the key approximation result!</strong></p>
    </div>

    <hr>

    <h2>4. Proof of Interpolation Error Bound</h2>
    
    <h3>4.1 Element-by-Element Analysis</h3>
    <p><strong>Proof strategy:</strong> Work on 1 element, then sum up.</p>

    <p>We need to show:</p>
    <p>$$\int_{x_{j-1}}^{x_j} (u - \pi u)'^2 \, dx \leq C(x_j - x_{j-1})^2 \int_{x_{j-1}}^{x_j} u''^2 \, dx$$</p>

    <p><strong>Let</strong> $e = u - \pi u$ be the error. Note that $u'' = e''$ since $\pi u$ is piecewise linear ($(\pi u)'' = 0$ on each element).</p>

    <h3>4.2 Change of Variables</h3>
    <p>By change of variables:</p>
    <p>$$x = x_{j-1} + \tilde{x}(x_j - x_{j-1})$$</p>

    <p>We rewrite the problem on the reference element $[0,1]$:</p>
    <p>$$\int_{x_{j-1}}^{x_j} e'^2 \, dx \leq C(x_j - x_{j-1})^2 \int_{x_{j-1}}^{x_j} e''^2 \, dx$$</p>

    <p><strong>This is equivalent to:</strong></p>
    <p>$$\int_0^1 e'^2 \, d\tilde{x} \leq C \int_0^1 e''^2 \, d\tilde{x}$$</p>

    <h3>4.3 Using Rolle's Theorem</h3>
    <p><strong>Rolle's Theorem:</strong> If $e$ is continuous on $[a,b]$ and $e(a) = e(b)$, then there exists at least one point $c \in [a,b]$ such that $e'(c) = 0$.</p>

    <p>Since $e(0) = e(1) = 0$ (interpolant matches at endpoints), there exists $\xi \in [0,1]$ such that:</p>
    <p>$$e'(\xi) = 0$$</p>

    <h3>4.4 The Key Estimate</h3>
    <p>By the **Fundamental Theorem of Calculus**:</p>
    <p>$$e'(y) - e'(\xi) = \int_\xi^y e'' \, dx$$</p>

    <p>Since $e'(\xi) = 0$:</p>
    <p>$$|e'(y)|^2 = \left|\int_\xi^y e'' \, dx\right|^2$$</p>

    <p><strong>Apply Cauchy-Schwarz:</strong></p>
    <p>$$\begin{aligned}
    |e'(y)|^2 &= \left|\int_\xi^y 1 \cdot e'' \, dx\right|^2 \\
    &\leq \left(\int_\xi^y 1^2 \, dx\right) \left(\int_\xi^y (e'')^2 \, dx\right) \\
    &= |y - \xi| \int_\xi^y (e'')^2 \, dx \\
    &\leq |y - \xi| \int_0^1 (e'')^2 \, dx
    \end{aligned}$$</p>

    <h3>4.5 Integration Over the Element</h3>
    <p>Integrating over $y \in [0,1]$:</p>
    <p>$$\int_0^1 (e'(y))^2 \, dy \leq \int_0^1 |y - \xi| \, dy \cdot \int_0^1 (e'')^2 \, dx$$</p>

    <p><strong>Taking max over $\xi$</strong> gives worst-case scenario:</p>
    <p>$$\max_{\xi \in [0,1]} \int_0^1 |y - \xi| \, dy = \frac{1}{2}$$</p>
    <p>(achieved when $\xi = 1/2$)</p>

    <p><strong>Therefore:</strong></p>
    <p>$$\int_0^1 (e')^2 \, dy \leq \frac{1}{2} \int_0^1 (e'')^2 \, dx$$</p>

    <p><strong>And we're done!</strong> Taking $C = 1/2$ and summing over all elements completes the proof. $\quad \checkmark$</p>

    <hr>

    <h2>5. Convergence Rates Summary</h2>
    
    <h3>5.1 Main Results</h3>
    <p>We just showed $(A)$ holds with $\varepsilon = h$.</p>

    <p><strong>Therefore:</strong></p>

    <div class="theorem-box">
        <p><strong>(1) Energy norm error (quasi-optimality):</strong></p>
        <p>$$\|u - u_h\|_E \leq \|u - v\|_E \quad \forall v \in V_h$$</p>

        <p><strong>Picking $v = \pi u \in V_h$:</strong></p>
        <p>$$\boxed{\|u - u_h\|_E \leq C_1 h \|u''\| = C_1 h \|f\|}$$</p>

        <p><strong>(2) $L^2$ norm error (superconvergence via duality):</strong></p>
        <p>$$\boxed{\|u - u_h\| \leq C_2 h^2 \|u''\| = C_2 h^2 \|f\|}$$</p>
    </div>

    <p><strong>Key observation:</strong> $L^2$ error converges **twice as fast** (order $h^2$) compared to energy error (order $h$). This is **superconvergence**.</p>

    <h3>5.2 What's Up Next</h3>
    <p>Now that we understand classical FEM error analysis, we can extend to machine learning:</p>
    <ul>
        <li><strong>Machine learning $V_h$</strong> (learning the function space itself)</li>
        <li><strong>Machine learning $a(u,v)$</strong> (learning the bilinear form)</li>
    </ul>

    <p>First, an abstraction of what we learned today, so it's clear this isn't just something special about Poisson...</p>

    <hr>

    <h2>6. Lax-Milgram Theory</h2>
    
    <h3>6.1 General Framework</h3>
    <p><strong>There isn't anything special about Poisson</strong>—the same process holds for any elliptic bilinear form.</p>

    <p>Let $V$ be a **Hilbert space** with norm $\|\cdot\|_V$.</p>

    <p>Suppose $a(u,v)$ satisfies:</p>

    <p><strong>(1) Symmetric:</strong></p>
    <p>$$a(u,v) = a(v,u)$$</p>

    <p><strong>(2) Continuous</strong> (have a Cauchy-Schwarz):</p>
    <p>There exists $\gamma > 0$ such that:</p>
    <p>$$|a(v,w)| \leq \gamma \|v\|_V \|w\|_V \quad \forall v, w \in V$$</p>

    <p><strong>(3) Elliptic</strong> (aka $V$-elliptic, coercive):</p>
    <p>There exists $\alpha > 0$ such that:</p>
    <p>$$a(v,v) \geq \alpha \|v\|_V^2 \quad \forall v \in V$$</p>

    <h3>6.2 Lax-Milgram Theorem</h3>
    <div class="theorem-box">
        <p><strong>Theorem:</strong> If $L: V \to \mathbb{R}$ is a bounded linear functional with:</p>
        <p>$$|L(v)| \leq \Lambda \|v\|_V$$</p>

        <p>Then:</p>

        <p><strong>(1) Existence & Uniqueness:</strong> The problem</p>
        <p>$$a(u,v) = L(v) \quad \forall v \in V$$</p>
        <p>has a **unique solution** $u \in V$.</p>

        <p><strong>(2) Stability estimate:</strong></p>
        <p>$$\boxed{\|u\|_V \leq \frac{\Lambda}{\alpha}}$$</p>

        <p><strong>(3) Equivalence to energy minimization:</strong></p>
        <p>$$u = \arg\min_{v \in V} F(v)$$</p>
        <p>where $F(v) = \frac{1}{2}a(v,v) - L(v)$.</p>
    </div>

    <h3>6.3 Proof Sketch</h3>
    <p><strong>Min $F(v) \Rightarrow a(u,v) = L(v)$:</strong> Immediate from $\delta_v F(v) = 0$.</p>

    <p><strong>Stability estimate:</strong> Take $v = u$:</p>
    <p>$$a(u,u) = L(u)$$</p>

    <p>By coercivity and boundedness:</p>
    <p>$$\alpha \|u\|_V^2 \leq a(u,u) = L(u) \leq \Lambda \|u\|_V$$</p>

    <p>Therefore:</p>
    <p>$$\|u\|_V \leq \frac{\Lambda}{\alpha} \quad \checkmark$$</p>

    <p><strong>Uniqueness:</strong> Assume two solutions $u_1, u_2$:</p>
    <p>$$a(u_1 - u_2, v) = 0 \quad \forall v \in V$$</p>

    <p>Take $v = u_1 - u_2$:</p>
    <p>$$a(u_1 - u_2, u_1 - u_2) = 0$$</p>

    <p>By coercivity:</p>
    <p>$$\alpha \|u_1 - u_2\|_V^2 \leq 0$$</p>

    <p>Therefore:</p>
    <p>$$\|u_1 - u_2\|_V = 0 \quad \Rightarrow \quad u_1 = u_2 \quad \checkmark$$</p>

    <hr>

    <h2>7. The General FEM Playbook</h2>
    <p>For **any elliptic operator**, we have the playbook:</p>

    <ol>
        <li><strong>Prove Lax-Milgram conditions</strong> (continuity, coercivity)<br>
        → Guarantees existence, uniqueness, stability</li>

        <li><strong>Get quasi-optimality</strong> for best fit in $\|\cdot\|_V$:
        $$\|u - u_h\|_V \leq C \inf_{v \in V_h} \|u - v\|_V$$</li>

        <li><strong>Construct interpolant</strong> as upper bound:
        $$\inf_{v \in V_h} \|u - v\|_V \leq \|u - \pi u\|_V$$</li>

        <li><strong>Prove interpolation error bounds</strong>:
        $$\|u - \pi u\|_V \leq C h^k \|u^{(k+1)}\|$$
        where $k$ depends on the polynomial degree of $V_h$.</li>

        <li><strong>Apply duality argument</strong> to relate $\|\cdot\|_V$ to $\|\cdot\|_{L^2}$ and understand convergence rates.</li>
    </ol>

    <p><strong>This framework extends to:</strong></p>
    <ul>
        <li>Higher dimensions</li>
        <li>Vector-valued problems (elasticity, fluid dynamics)</li>
        <li>Mixed formulations</li>
        <li>Nonlinear problems</li>
        <li><strong>Learning both $V_h$ and $a(u,v)$ in ML contexts</strong></li>
    </ul>

    <hr>

    <h2>8. Summary</h2>
    <div class="summary-box">
        <p>This lecture covered:</p>

        <ol>
            <li><strong>Bilinear forms</strong> and <strong>energy norms</strong> as abstractions of Galerkin methods</li>
            <li><strong>Quasi-optimality</strong> in energy norm: FEM gives best approximation in $V_h$</li>
            <li><strong>Duality argument</strong> (Aubin-Nitsche trick) for $L^2$ error estimates</li>
            <li><strong>Interpolation theory</strong>: constructing $\pi u$ and bounding approximation error</li>
            <li><strong>Convergence rates</strong>: $\mathcal{O}(h)$ in energy norm, $\mathcal{O}(h^2)$ in $L^2$ norm</li>
            <li><strong>Lax-Milgram theory</strong>: general framework for elliptic problems</li>
            <li><strong>FEM playbook</strong>: systematic approach to error analysis for any elliptic operator</li>
        </ol>

        <p><strong>Key Takeaway:</strong> The power of finite element methods lies not just in their flexibility for complex geometries, but in their **provable optimal approximation properties**. The Lax-Milgram framework shows that for any elliptic bilinear form satisfying continuity and coercivity, we obtain existence, uniqueness, stability, and convergence with quantifiable error bounds. The duality argument reveals superconvergence in $L^2$ norm—convergence twice as fast as in the energy norm. This theoretical foundation is essential for physics-informed machine learning: when we learn function spaces $V_h$ or operators $a(u,v)$, we need to ensure these learned components preserve the mathematical structure (symmetry, coercivity, continuity) that guarantees convergence.</p>
    </div>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>
</body>
</html>